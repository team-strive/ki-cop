{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59f83908-074a-44b4-bf5c-3f149a4db3aa",
   "metadata": {},
   "source": [
    "# Topic Clustering Experiment\n",
    "## Overview\n",
    "Uses data created from [download-reddit-data](./download-reddit-data.ipynb) and [create-embeddings](./create-embeddings.ipynb) notebooks to\n",
    " * generate topic clusters from embedding vectors using a clustering algorithm from *scikit-learn*\n",
    " * retrieve representative text for each cluster\n",
    " * query LLM for topic description texts (~tags) for each cluster using those texts\n",
    " * connect the topics to a topic graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc77a07-261e-439d-ba7e-ef52ce43c9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "import chromadb\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from adjustText import adjust_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62183ad4-cc6e-4065-9e94-330bacf13501",
   "metadata": {},
   "source": [
    "## Experiment Data\n",
    "### Input/Output Setup\n",
    "Loads *pickled* data from the [create-embeddings](./create-embeddings.ipynb) notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42417504-7df3-490b-925f-cc490a87b5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLITS_FILE = \"reddit-splits-1000-200.pickle\"\n",
    "VECS_FILE = \"reddit-vecs-1000-200.pickle\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db91f856-ebb0-4e9d-b18f-a718ccba405b",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467b9b0f-1cd4-44f8-90fa-ded645b0c29b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(SPLITS_FILE, \"rb\") as file:\n",
    "    splits = pickle.load(file)\n",
    "with open(VECS_FILE, \"rb\") as file:\n",
    "    vecs = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41a3ef45-4c50-47ce-9a31-dec9d5fc7a6d",
   "metadata": {},
   "source": [
    "## Clustering\n",
    "Generate *topic clusters* from embedding vectors using a clustering algorithm from *scikit-learn*.\n",
    "\n",
    "**Please note**: the ability to cluster the embedding vectors might somehow depend on how the embedding function works. For example, there are embeddings known to work better with certain similarity measures like *cosine similarity*.\n",
    "\n",
    "**Experiment potential**: the generated clusters also heavily depend on the used algorithm and algorithm-specific parameters. It's also possible to influence the number of output topics in this step. The provided code is only some naive first shot to pull together everything.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/clustering.html#clustering provides a good overview of clustering algorithms available in *scikit-learn*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cd6663-e047-4e3a-bc7c-53ff7f91ff4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanShift\n",
    "from sklearn.cluster import MeanShift, estimate_bandwidth\n",
    "bandwidth = estimate_bandwidth(vecs, quantile=0.2, n_samples=len(vecs))\n",
    "clusterer = MeanShift(bandwidth=bandwidth*1.1, n_jobs=-1, cluster_all=False)\n",
    "\n",
    "# some alternatives for clustering\n",
    "\n",
    "#from sklearn.cluster import DBSCAN\n",
    "#clusterer = DBSCAN(eps=.5, min_samples=3)\n",
    "\n",
    "#from sklearn.cluster import KMeans\n",
    "#clusterer = KMeans(n_clusters=10)\n",
    "\n",
    "clusters = clusterer.fit(vecs)\n",
    "\n",
    "labels = clusters.labels_\n",
    "\n",
    "# Number of clusters in labels, ignoring noise if present.\n",
    "n_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "n_noise_ = list(labels).count(-1)\n",
    "\n",
    "print(\"Estimated number of clusters: %d\" % n_clusters_)\n",
    "print(\"Estimated number of noise points: %d\" % n_noise_)\n",
    "print(f\"Labels: {set(labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d56ee9-3ebd-4672-a55f-08c13c99b43d",
   "metadata": {},
   "source": [
    "## Topic Creation\n",
    "The general idea is to map back topic labels (output of clustering) to text, which serve a prompt input for some LLM. The LLM is asked to produce small topic headlines (~tags) for each cluster.\n",
    "\n",
    "### Retrieve Text for Topics\n",
    "To retrieve *representative text* given a topic cluster, the natural choice is a vector DB. In this notebook, *ChromaDB* is used: https://docs.trychroma.com/\n",
    "\n",
    "The approach is as follows:\n",
    " * Init a ChromaDB and index all document *splits* using their previously computed embeddings\n",
    " * Compute vectors representing each topic cluster (centroid of all vectors forming the topic/cluster)\n",
    " * Query text from ChromaDB for each vector representing a topic cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f502d1-e119-4529-ac08-851be5f6f81b",
   "metadata": {},
   "source": [
    "#### ChromaDB Text Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe513a9-eca3-4710-9f15-2362f9b318c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get back texts for labeled vecs stored in vector store\n",
    "chroma_client = chromadb.EphemeralClient()\n",
    "collection = chroma_client.create_collection(name=\"docs\")\n",
    "ids=[str(i) for i in range(len(splits))]\n",
    "collection.add(\n",
    "    documents=[d.page_content for d in splits],\n",
    "    embeddings=vecs,\n",
    "    # trouble with some metadata types\n",
    "    #metadatas=[d.metadata for d in splits],\n",
    "    ids=ids\n",
    ")\n",
    "print(f\"{collection.count()} docs added to Chroma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ab25c-2ae9-423a-92fb-043d513d751e",
   "metadata": {},
   "source": [
    "#### Representative Topic Vectors\n",
    "**Experimentation potential**: computing the centroid of vectors belonging to a cluster is just one possible approach to tackle this problem. In particular, it keeps this experiment notebook simple, but might not be the best solution to this, depending on the clustering algorithm used and also depending on the produced cluster output. For example, one could also define some kind of boundary volume and collect representative vectors from this to be used in the next step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb3aa3c-a702-4611-845c-5549f1738f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_labels = set([l for l in labels if l >= 0])\n",
    "# get representative vec for all labels (centroid)\n",
    "representatives = []\n",
    "for label in unique_labels:\n",
    "    vecs_with_label = np.array([v for ix, v in zip(range(len(vecs)), vecs) if labels[ix] == label])\n",
    "    centroid = np.mean(vecs_with_label, axis=0).tolist()\n",
    "    representatives.append(centroid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7637130-4630-41f1-9f76-ac03cf1f2ca7",
   "metadata": {},
   "source": [
    "#### Representative Topic Text\n",
    "**Experimentation potential**: the following code retrieves exactly one document split per topic vector. This inherently has some issues:\n",
    " * multiple splits could be concatenated to fill the LLM context window more efficiently\n",
    " * picking one split might yield bad results, if this split is not very elaborated (for example, it contains headlines only in some cases)\n",
    " * multiple splits could be picked from multiple vectors for more variance (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c53c5bf2-d3d0-474b-bad2-0f53198849cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "representative_docs = collection.query(\n",
    "    query_embeddings=representatives,\n",
    "    n_results=1,\n",
    "    include=[\"documents\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066e813b-dece-4f99-ac20-b250dbb2a6e1",
   "metadata": {},
   "source": [
    "### Generate Topic Text\n",
    "Use representative text and some system prompt to let LLM generate *topc descriptions* per cluster.\n",
    "\n",
    "**Experimentation potential**:\n",
    " * Different LLMs\n",
    " * Different system prompts\n",
    " * Combine multiple representative text (see above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0fa861f-658b-48f0-a4a8-dd295794f2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ask LLM for summary/tag\n",
    "llm = Ollama(model=\"llama3\")\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Summarize in maximum three words. No other output. No punctuation.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | llm | output_parser\n",
    "\n",
    "tags = [chain.invoke({ \"input\": docs[0] }) for docs in tqdm(representative_docs[\"documents\"])]\n",
    "print(tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767c3939-c01d-449d-9fe8-3efee3958ba4",
   "metadata": {},
   "source": [
    "## Topic Graph\n",
    "Up to this step, we have a set of topic clusters associated with their respective topic vector in embedding space and generated topic text. The overall aim is to structure them in some kind of knowledge graph.\n",
    "\n",
    "Natural candidates to solve this are graph algorithms. Each topic (text + vector) can be represented as nodes. The vectors can be used to compute some kind of edge *costs* in a fully connected undirected graph (complete graph), for example by utilizing some *similarity measure*, similarly to what a vector database does for retrieving relevant documents given a query vector.\n",
    "\n",
    "### Create\n",
    "This notebook computes a *Minimum Spanning Tree* (MST) using *SciPy* and using the general approach described above. Edge costs are modeled as node *similarities* using *cosine measure*.\n",
    "\n",
    "**Experimentation potential**:\n",
    " * MST is only a naive first shop. Obviously, this will output a *tree*, but a more generic *graph* representation might be better suitable\n",
    " * Different models for edge costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f42bc0-062a-4385-9d55-e3aa99988095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create graph from tags and vecs\n",
    "from scipy.sparse.csgraph import minimum_spanning_tree\n",
    "\n",
    "# cosine sim for each (vec, other) with other in vecs\n",
    "def similarities(vec, vecs):\n",
    "    return np.array([np.dot(vec, other_vec) / (np.linalg.norm(vec) * np.linalg.norm(other_vec)) for other_vec in vecs])\n",
    "\n",
    "adjacency_matrix = np.array([1 - similarities(vec, representatives) for vec in representatives])\n",
    "# scale to 1\n",
    "adjacency_matrix /= np.max(adjacency_matrix)\n",
    "\n",
    "mst = minimum_spanning_tree(adjacency_matrix)\n",
    "edges = (mst.toarray() > 0).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2770268d-7cdd-40e1-8d15-198beb601262",
   "metadata": {},
   "source": [
    "### Vizualize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3caebf-e2a6-4217-ae68-ad2fb3874f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.from_numpy_array(edges)\n",
    "nodes = { label: tag for label, tag in zip(unique_labels, tags) }\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 9))\n",
    "pos = nx.spring_layout(G)  # Positioning of nodes\n",
    "# Draw nodes and edges\n",
    "nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=250)\n",
    "nx.draw_networkx_edges(G, pos, edge_color='gray')\n",
    "\n",
    "# Draw labels\n",
    "texts = []\n",
    "for node, (x, y) in pos.items():\n",
    "    texts.append(plt.text(x, y, nodes[node], fontsize=8, ha='center', va='center'))\n",
    "\n",
    "# Adjust text to avoid overlap\n",
    "adjust_text(texts)\n",
    "\n",
    "# Set the title\n",
    "plt.title(\"Topic network\")\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
